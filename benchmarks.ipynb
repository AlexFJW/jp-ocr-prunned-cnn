{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarks for trained networks"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.model_utils import benchmark\n",
        "from src.data.dataloaders import *\n",
        "from src.nn.models import *\n",
        "import torch"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unpruned networks:\n",
        "\n",
        "### VGG with transfer learning\n",
        "Model: VGG_11, with batch normalization  \n",
        "Training data: etl2 \n",
        "\n",
        "Training Parameters:  \n",
        "Optimizer:  Stoichastic Gradient Descent(SGD), learning rate=0.001, momentum=0.9, Step-LR with step_size=7 & gamma=0.1  \n",
        "Loss function: Cross Entropy Loss\n",
        "\n",
        "#### Summary of results:   \n",
        "Size: 1101.2MB  \n",
        "F1(micro): 0.000  \n",
        "Inference time per image:   \n",
        "8.25s (PC)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# perform benchmark on vgg\n",
        "etl2, etl2_classes = get_etl2_dataloaders('vgg11_bn')\n",
        "model, _ = vgg_model(etl2_classes)\n",
        "model.load_state_dict(torch.load('trained_models/vgg11_bn_etl2.weights'))\n",
        "print(model)\n",
        "benchmark(model, etl2['test'], 'vgg11_bn')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "restored pickled etl2 data\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
            "    (4): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (6): ReLU(inplace)\n",
            "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
            "    (8): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (10): ReLU(inplace)\n",
            "    (11): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (13): ReLU(inplace)\n",
            "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
            "    (15): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (17): ReLU(inplace)\n",
            "    (18): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (20): ReLU(inplace)\n",
            "    (21): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
            "    (22): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (24): ReLU(inplace)\n",
            "    (25): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (27): ReLU(inplace)\n",
            "    (28): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Dropout(p=0.5)\n",
            "    (3): Linear(in_features=4096, out_features=4096)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.5)\n",
            "    (6): Linear(in_features=4096, out_features=2168)\n",
            "  )\n",
            ")\n",
            "Benchmark, vgg11_bn: \n",
            "Size of model: 1101.233088 MB\n",
            "Loss: 0.2373\n",
            "Micro Precision: 0.0060 Recall: 0.0060 F1: 0.0060\n",
            "Macro Precision: 0.0000 Recall: 0.0005 F1: 0.0000\n",
            "Time taken for inference: 8.244925s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330/330 [00:53<00:00,  6.21it/s]\n",
            "/Users/alex/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chinese OCR inspired network\n",
        "Model: ChineseNet, from (https://arxiv.org/abs/1702.07975)  \n",
        "\n",
        "#### Type A:  \n",
        "Training data: etl2\n",
        "\n",
        "Training Parameters:  \n",
        "Optimizer:  Stoichastic Gradient Descent(SGD), learning rate=0.001, momentum=0.9, Step-LR with step_size=7 & gamma=0.1  \n",
        "Loss function: Cross Entropy Loss\n",
        "\n",
        "#### Summary of results:  \n",
        "Size: 58.15 MB  \n",
        "F1(micro): 0.9913  \n",
        "Inference time per image: 1.73s  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# perform benchmark on chinese_net\n",
        "etl2, etl2_classes = get_etl2_dataloaders('chinese_net')\n",
        "model, _ = chinese_model(etl2_classes)\n",
        "model.load_state_dict(torch.load('trained_models/chinese_net_etl2.weights'))\n",
        "print(model)\n",
        "\nbenchmark(model, etl2['test'], 'chinese_net_etl2')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "restored pickled etl2 data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/330 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChineseNet(\n",
            "  (features): Sequential(\n",
            "    (0): PConv2d (1, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (4): PConv2d (96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): PBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (6): PReLU(num_parameters=1)\n",
            "    (7): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (8): PConv2d (128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): PBatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (10): PReLU(num_parameters=1)\n",
            "    (11): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (12): PConv2d (160, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): PBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (14): PReLU(num_parameters=1)\n",
            "    (15): PConv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): PBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (17): PReLU(num_parameters=1)\n",
            "    (18): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (19): PConv2d (256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): PBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (21): PReLU(num_parameters=1)\n",
            "    (22): PConv2d (384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): PBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (24): PReLU(num_parameters=1)\n",
            "    (25): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): PLinear(in_features=1536, out_features=1024)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.5)\n",
            "    (4): Linear(in_features=1024, out_features=2168)\n",
            "  )\n",
            ")\n",
            "Benchmark, chinese_net_etl2: \n",
            "Size of model: 58.153728 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330/330 [00:10<00:00, 32.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0018\n",
            "Micro Precision: 0.9913 Recall: 0.9913 F1: 0.9913\n",
            "Macro Precision: 0.9904 Recall: 0.9882 F1: 0.9879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/Users/alex/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for inference: 1.726826s\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Type B:  \n",
        "Training data: etl2 + etl9g  \n",
        "All other details are identical as Type A  \n",
        "#### Summary of results:  \n",
        "Size: 58.15 MB  \n",
        "F1(micro): 0.9913  \n",
        "Inference time per image (PC):  1.55s   \n",
        "Inference time per image (Mobile):  At least 11.24s "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# perform benchmark on chinese_net\n",
        "etl2_9g, etl2_9g_num_classes = get_etl2_9g_dataloaders('chinese_net')\n",
        "model, _ = chinese_model(etl2_9g_num_classes)\n",
        "model.load_state_dict(torch.load('trained_models/chinese_net_etl2_9g.weights'))\n",
        "benchmark(model, etl2_9g['test'], 'chinese_net_etl2_9g')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "restored pickled etl2 data\n",
            "processing raw etl9g data\n",
            "Benchmark, chinese_net_etl2_9g: \n",
            "Size of model: 66.255328 MB\n",
            "Loss: 0.0006\n",
            "Micro Precision: 0.9963 Recall: 0.9963 F1: 0.9963\n",
            "Macro Precision: 0.9962 Recall: 0.9958 F1: 0.9958\n",
            "Time taken for inference: 1.550738s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4125/4125 [01:57<00:00, 35.06it/s]\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruned networks:\n",
        "\n",
        "Model: ChineseNet  \n",
        "~80% convolutional feature maps pruned  \n",
        "Training data: etl2 + etl9g  \n",
        "\n",
        "#### Summary of results:  \n",
        "Size: 31.22 MB  \n",
        "F1(micro): 0.989  \n",
        "Inference time per image (PC):  0.364s   \n",
        "Inference time per image (Mobile):  At least 2.64s "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# perform benchmark on chinese_net_80\n",
        "model, _ = chinese_pruned_80(etl2_9g_num_classes)\n",
        "print(model)\n",
        "model.load_state_dict(torch.load('trained_models/pruned_chinese_net_etl2_9g_80p_ft250.weights'))\n",
        "benchmark(model, etl2_9g['test'], 'prunned_chinese_net_80')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4125/4125 [01:31<00:00, 45.23it/s]\n",
            "/Users/alex/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChineseNet(\n",
            "  (features): Sequential(\n",
            "    (0): PConv2d (1, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PBatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (4): PConv2d (26, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): PBatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (6): PReLU(num_parameters=1)\n",
            "    (7): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (8): PConv2d (39, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): PBatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (10): PReLU(num_parameters=1)\n",
            "    (11): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (12): PConv2d (52, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): PBatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (14): PReLU(num_parameters=1)\n",
            "    (15): PConv2d (75, 93, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): PBatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (17): PReLU(num_parameters=1)\n",
            "    (18): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (19): PConv2d (93, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): PBatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (21): PReLU(num_parameters=1)\n",
            "    (22): PConv2d (88, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): PBatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (24): PReLU(num_parameters=1)\n",
            "    (25): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): PLinear(in_features=380, out_features=1024)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.5)\n",
            "    (4): Linear(in_features=1024, out_features=3156)\n",
            "  )\n",
            ")\n",
            "Benchmark, prunned_chinese_net_80: \n",
            "Size of model: 31.223096 MB\n",
            "Loss: 0.0014\n",
            "Micro Precision: 0.9890 Recall: 0.9890 F1: 0.9890\n",
            "Macro Precision: 0.9875 Recall: 0.9860 F1: 0.9863\n",
            "Time taken for inference: 0.364131s\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: ChineseNet  \n",
        "~90% convolutional feature maps pruned  \n",
        "Training data: etl2 + etl9g  \n",
        "\n\n",
        "#### Summary of results:  \n",
        "Size: 27.15 MB  \n",
        "F1(micro): 0.9573\n",
        "Inference time per image (PC): 0.204s   \n",
        "Inference time per image (Mobile):  At least 1.48s"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# perform benchmark on chinese_net_90\n",
        "model, _ = chinese_pruned_90(etl2_9g_num_classes)\n",
        "print(model)\n",
        "model.load_state_dict(torch.load('trained_models/pruned_chinese_net_etl2_9g_90p_ft250.weights'))\n",
        "benchmark(model, etl2_9g['test'], 'prunned_chinese_net_90')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4125/4125 [01:30<00:00, 45.80it/s]\n",
            "/Users/alex/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChineseNet(\n",
            "  (features): Sequential(\n",
            "    (0): PConv2d (1, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PBatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (4): PConv2d (15, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): PBatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (6): PReLU(num_parameters=1)\n",
            "    (7): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (8): PConv2d (14, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): PBatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (10): PReLU(num_parameters=1)\n",
            "    (11): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (12): PConv2d (20, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): PBatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (14): PReLU(num_parameters=1)\n",
            "    (15): PConv2d (27, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): PBatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (17): PReLU(num_parameters=1)\n",
            "    (18): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "    (19): PConv2d (31, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): PBatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (21): PReLU(num_parameters=1)\n",
            "    (22): PConv2d (28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): PBatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (24): PReLU(num_parameters=1)\n",
            "    (25): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): PLinear(in_features=120, out_features=1024)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.5)\n",
            "    (4): Linear(in_features=1024, out_features=3156)\n",
            "  )\n",
            ")\n",
            "Benchmark, prunned_chinese_net_90: \n",
            "Size of model: 27.14932 MB\n",
            "Loss: 0.0050\n",
            "Micro Precision: 0.9573 Recall: 0.9573 F1: 0.9573\n",
            "Macro Precision: 0.9493 Recall: 0.9444 F1: 0.9448\n",
            "Time taken for inference: 0.204223s\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix  \n",
        "\n",
        "Micro vs Macro for precision, recall, F1:  \n",
        "The terms are exactly as defined in sklearn.  \n",
        "Micro calculates precision, recall and F1 scores over all results.  \n",
        "Macro calculates precision, recall and F1 scores over each class, and averages the results.  \n",
        "\n",
        "Test hardware:  \n",
        "CPU for inference: Intel i3-3140  \n",
        "GPU for training: GTX 1060 6GB  \n",
        "Mobile CPU: Octa-core (4x Cortex-A53 & 4x Cortex-A53), a low end CPU from ~2015.  \n",
        "\n",
        "Estimating model size:  \n",
        "Model size was estimated by summing the size of parameters in each model.  \n",
        "Each float32 takes 4 bytes.  \n",
        "For reference, squeezenet takes about 12.76MB.  \n",
        "\n",
        "Estimating mobile performance:  \n",
        "Squeezenet takes about 0.566s on the Cortex-A53,  \n",
        "and 0.0781s on the Intel i3.  \n",
        "The Intel i3 performs about performs 7.25x faster, and will be used to estimate performance on mobile. Caffe2 is unfortunately not mature enough for benchmarking different networks.  \n",
        "\n",
        "Data used:   \n",
        "Etl2 - machine printed, contains less classes & samples than Etl9g  \n",
        "Etl9g - human written, font size varies slightly among samples, font is smaller than Etl2.  \n",
        "Train, validation & test split:  \n",
        "Data was split into 3 groups, stratified by class. Test(20%), Validation(16%), Train(64%) \n",
        "\n",
        "Training approach:  \n",
        "Models were trained on the train set.  \n",
        "Early stopping was used, picking the model with least loss against the validation set.  \n",
        "Performance of the models are evaluated on the test set for this benchmark.  \n",
        "\n",
        "Pruning approach:  \n",
        "All conv2d feature maps are valid candidates for pruning.  \n",
        "Feature maps are evaluated by their taylor importance, and the least important feature map is discarded.  \n",
        "Each pruning step is followed with 250 iterations of finetuning to restore model accuracy.    "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}